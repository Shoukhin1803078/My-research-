
## Task Overview
This take-home test aims to assess my ability to identify and select relevant datasets and models for specific tasks, understand and explain the capabilities and limitations of Large Language Models (LLMs), demonstrate technical skills and knowledge in Python and machine learning concepts, and effectively communicate findings and reasoning through clear and concise explanations.

## Dataset Selection
For this task, I selected the "News Category Dataset" from Kaggle. The dataset contains over 200,000 news articles categorized into different topics such as politics, entertainment, sports, etc. I chose this dataset because:

- **Data Size**: The dataset is sufficiently large to train or fine-tune Large Language Models (LLMs) effectively.
- **Relevance to Real-World Applications**: News articles reflect real-world events and topics, making the dataset relevant to natural language processing tasks.
- **Potential for Creative Exploration**: The diverse range of news categories allows for creative exploration using prompt engineering techniques.

## Relevant Kaggle Model
After selecting the dataset, I searched for existing models on Kaggle that utilize Large Language Models (LLMs) or similar techniques for text analysis tasks. I identified the "Google Gemma" model as a relevant option. The Gemma model is a pre-trained LLM that can be fine-tuned for various natural language processing tasks. Reasons for choosing the Gemma model include:

- **Capability for Text Analysis Tasks**: The Gemma model has demonstrated state-of-the-art performance on tasks such as text generation, sentiment analysis, and question answering, aligning with the objectives of the "News Category Dataset".
- **Availability and Accessibility**: The Gemma model is available on Kaggle and can be easily accessed for experimentation and fine-tuning.

## Evaluation of Model's Capabilities
The Gemma model offers impressive capabilities for natural language understanding and generation tasks. It achieves state-of-the-art performance on benchmarks such as text generation, question answering, and sentiment analysis. However, the model's limitations include computational resource requirements for training and fine-tuning, as well as potential biases present in the pre-trained weights.

## Repository Contents
- `README.md`: This file provides an overview of the task, dataset selection, model identification, and evaluation of model capabilities.
- `notebook.ipynb`: Jupyter Notebook documenting the steps taken for dataset selection, model identification, and evaluation.

## GitHub Repository
The code and documentation for this take-home test can be found in the following public GitHub repository: [GitHub Repository Link](https://github.com/yourusername/take-home-test-bdmli)

## Documentation and References
To supplement my work, I referenced the following documentation and tutorials:
- [Fine-Tuning LLM with Gemma 2](https://www.datacamp.com/tutorial/fine-tuning-llama-2)
- [Your Ultimate Guide to Instinct Fine-Tuning and Optimizing Google's Gemma 2b using Lora](https://medium.com/@mohammed97ashraf/your-ultimate-guide-to-instinct-fine-tuning-and-optimizing-googles-gemma-2b-using-lora-51ac81467ad2)
- [Hands-On with Fine-Tuning LLM](https://www.labellerr.com/blog/hands-on-with-fine-tuning-llm/)

## Submission
The GitHub repository link has been submitted within the specified timeframe to the designated platform for evaluation.



Learning from the training operational observation:

    Llama2 finetunes faster. This is likely because Llama2â€“7b is a smaller than gemma-7b.
    Llama2 shows better training loss on this finetuning task. Llama2 fits the finetuning data a lot better, but it may also subject to overfitting faster as training epochs increase)
    Llama2 outperforms in terms of loading and responding
    Llama2 responses a bit faster than Gemma. The response time highly depends on the number of generated tokens. The longer the response, the slower the inference. For my example questions tested on NVIDIA A10G 24G, inference time spans from 0.2s to 40s.
